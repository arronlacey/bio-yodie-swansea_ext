
Learning Framework PR
=====================

The Learning Framework PR is GATE's most recent machine learning PR. It offers a wider variety of more up to date ML algorithms than earlier machine learning PRs, currently supporting several Weka classification algorithms, various Mallet classification algorithms, Mallet's CRF implementation and LibSVM. It offers broadly the same functionality as the Batch Learning PR, with some differences--in addition to providing a broader range of algorithms, it is likely to be faster to train and apply under most circumstances, export to sparse ARFF format is included, and the interface design is a little different, offering more settings in the form of runtime parameters, and supporting multiple trained models in a more user-friendly way.

As in the Batch Learning PR, the Learning Framework PR implements different task modes:

- Classification, which simply assigns a class to each instance annotation. For example, each sentence might be classified as positive or negative;

- Named entity recognition, which finds entity mentions, such as locations or persons, within the text.

The next section provides some information for users considering switching from using the Batch Learning PR, or deciding which to prefer. Below that, the "Getting Started" section is a good place to start for users new to machine learning in GATE.


For Batch Learning PR Users
---------------------------

The Learning Framework PR is similar in style to the Batch Learning PR, and offers many similar features, but also differs in some ways. If you are considering switching from using the Batch Learning PR, there are several ways in which you will find the new PR different, and will need to change your application:

- If you are using the PR to learn relations, you will not find the Learning Framework PR suitable, because relations learning is not currently included;

- The configuration file is similar in style but different in some ways. Many settings are now runtime parameters, so you will need to include these in your app. Aside from that, some of the feature specifications will require small edits. Aside from relations features, everything is included, but in some cases, differently specified;

- If you want to use PAUM, the Batch Learning PR remains the only implementation;

- Evaluation is differently implemented in the Learning Framework PR. Classification accuracy is provided, facilitating parameter tuning, using both cross-validation and hold-out methods, but task evaluation is left to other GATE tools to provide, for example the Cross Validation PR, or Corpus QA.


Getting Started
---------------

To get started using machine learning in GATE, you need to decide which type of task you are interested in doing. Are you interested in finding things in the text, such as person names, or names of locations? If so, you are doing a named entity recognition task. If, however, you want to label each sentence, or each document, with some class (positive or negative would be common in sentiment analysis tasks), then you have a classification task. Examples for each task are provided below, but in this section, we'll cover the basic work flow.

The corpus is central to any attempt to apply machine learning. You will need to obtain and prepare a good quality corpus. Ideally your corpus should contain at least thousands of training instances; for simple tasks, some users have been successful with just a few hundred, but a more complex task may require tens or hundreds of thousands to produce a good result.

The corpus needs to be annotated by humans, so as to produce a "gold standard". These "right answers" are what the machine learning system will attempt to replicate on unseen data. For named entity recognition, this involves going through the corpus and putting annotations of a particular type across all occurrences of the target entity type; for example, annotating all person names with an annotation of type "Person", perhaps in the default annotation set. For classification tasks, the annotations will cover each span of interest, for example every sentence or every document, and the human annotator will apply a feature value to each one. for example, you might have "sentence" annotations in the default annotation set, and the human annotator might set the "type" feature of each one to "positive" or "negative".

Additionally, you need to have some useful features annotated on your corpus. For example, in finding city names, the string value of each token is likely to be useful. It is easy to see that being provided with information such as that the string of the token is "London" is likely to be useful to the learner in determining if it belongs in a city name or not! However, for many tasks, the inference process is more subtle and complex than that. The value in the machine learning approach lies in machine learners' ability to make different inferences than a human being would, were they simply writing JAPE rules to achieve their task. Our job at this stage, then, is to provide a variety of useful features, and let the learner figure out what it can use. A common approach to providing features is to run ANNIE over the corpus. Then we have access to information such as the string and type of each token, and so forth. You may also have additional features relevant to your task, that you can use, such as a gazetteer of positive or negative opinion words.

Next, you will write a feature specification file, describing the features you want to use. Some simple examples for each task type are provided below, and should be easily adaptable for your purposes. After that, you can load the Learning Framework plugin, and create a Learning Framework PR in your application. The Learning Framework PR has no init-time parameters. After you have created it, you can set the runtime parameters. Runtime parameters are described more fully in the next section, but in essence, you need to tell the PR what algorithm you want to use, where your feature specification is, where you want the model to be saved to, what your instance is ("Token" is common for NER tasks; "Sentence" would be common for classification), and how the learner will find the correct classes. For NER, class annotations take the form of the location of the "classType" annotation type; for classification, it is the "classFeature" on the "classType" that will be learned. Choose the right "mode" for your task, and set the "operation" to "TRAIN", before running the application.

Now you have a trained model that you can apply. Change "operation" to "APPLY_CURRENT_MODEL", and the model you have just trained will be applied by default. (If you trained other models earlier, you can change the save directory to indicate that you want to use this model instead.) By default, output annotations go in the "LearningFramework" annotation set, so you can look there and see what annotations the PR created. If it isn't what you expect, check your parameters and input annotations! The most common source of error is setting a parameter wrong, for example, the instance annotation, or not having the annotations present as you specified, for example, not having them all in the input annotation set.


Runtime Parameters
------------------

Unlike the Batch Learning PR, the Learning Framework takes much more information in the form of runtime parameters, and much less is required to be included in the configuration file. The configuration file defines only the learning attributes to be used. Examples are included below for each task type, and a section below describes the feature spec in more detail.

-classFeature--For classification, the class to be learned takes the form of a feature on the instance annotation. This should be specified here. In NER mode, this is ignored.

-classType--Indicates the annotation type from which the learning class should be taken. For classification, it is acceptable for the class to be taken from a different annotation type than the instance--the colocated classType annotation will be used. Often, however, this will just be the same as the instance type. For NER, this would indicate the annotation type, e.g. "Mention", that we are aiming to find.

-confidenceThreshold--for classification, this indicates the minimum confidence required from the model for us to create an annotation at application time. For NER, this indicates the minimum average confidence for the whole entity.

-featureSpecURL--indicates the location of the feature specification file. If you edit the actual content of the file then you need to reinitialize the PR for this change to register. There is a section below describing feature types.

-foldsForXVal--number of folds to use for cross-validation.

-identifierFeature--Mallet allows you to include an identifier feature in your training instances. It isn't used anywhere and it isn't terribly important.

-inputASName--contains instances, class annotations, sequence span annotations.

-instanceName--the annotation type to be used as training instance. Token is common.

-learnerParams--a free text field that some learners are able to make use of as parameters. See the section below for a specification of which learners take parameters and what they take.

-mode--NAMED_ENTITY_RECOGNITION or CLASSIFICATION

-operation:
 -TRAIN trains the model indicated in trainingAlgo (see below).
 -APPLY_CURRENT_MODEL applies whatever is found in the savedModel directory. It doesn't matter what is selected in trainingAlgo at apply time. It applies whatever it has.
 -EVALUATE_XFOLD--Cross-fold validation is performed and accuracy is returned.
 -EVALUATE_HOLDOUT--Hold-out validation is performed and accuracy is returned.
 -EXPORT_ARFF
 -EXPORT_ARFF_NUMERIC_CLASS--as above but where you have a numeric class rather than nominal.

-outputASName--Where to output the annotations to at application time.

-saveDirectory--indicates the location to which models should be saved, or the location from which to load the model you wish to apply. Allows you to switch between models at runtime.

-sequenceSpan--for sequence learning, the smallest meaningful sequence. This needs to be the name of an annotation type that appears in the document. "Sentence" would be common, for example in the context of an NER task where "Token" is instance. Training instances will be constructed for the entire sequence, as required by Mallet. Ignored for non-sequence learners.

-trainingAlgo--indicates the algorithm to use for training.

-trainingProportion--the proportion of the data to use for training in holdout evaluation.


Feature Specification
---------------------

The feature specification file is similar to the configuration file in the Batch Learning PR, except that it includes only specifications of what features (attributes) to use. All other configuration information is specified as a runtime parameter. Example feature specification files for the two task types are included below. The name of the file doesn't matter. In this section, we review each feature type and its usage.

Three feature types are specified; attributes, n-grams and attribute lists. Here is an example of an attribute specification, in which the string of token is used:

<ATTRIBUTE>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>
<DATATYPE>nominal</DATATYPE>
</ATTRIBUTE>

Attribute elements take features from annotations colocated with the instance, or typically, from the instance itself. Type and feature specify the annotation type and feature to use. Datatype specifies if the attribute is nominal or numeric. The string of token is clearly a nominal feature, whereas the length of token would be numeric. Note that nominal features expand the feature space much more than numeric ones, and string of token in particular is computationally expensive since it leads to a feature (dimension) being added for every unique string in the corpus. Category of token would be computationally cheaper.

In the case that more than one annotation of the specified type colocates with the instance, the first is used. In the case that multiple colocated annotations begin at the same offset, it is unclear which would be selected.

N-gram features are common in tasks such as sentence classification. Here is an example, in which a trigram for category of token is used:

<NGRAM>
<NUMBER>3</NUMBER>
<TYPE>Token</TYPE>
<FEATURE>category</FEATURE>
</NGRAM>

N-gram features create training attributes from features for all examples of the specified annotation type within the span of the instance. Number indicates the size of a sliding window across the attribute annotations within the instance. For a bag of words model you would simply specify "1", but larger n-grams can be used. Note that n-grams (n>1) lead to a larger number of features being generated than unigrams or attribute features, so using string of token with n>1 is something to be done with consideration.

Only the nominal datatype is supported for n-gram features, since numeric features in the context of an n-gram is not meaningful.

Here is an example attribute list, in which a range of orths of token are used to generate five features:

<ATTRIBUTELIST>
<TYPE>Token</TYPE>
<FEATURE>orth</FEATURE>
<DATATYPE>nominal</DATATYPE>
<FROM>-2</FROM>
<TO>3</TO>
</ATTRIBUTELIST>

Attribute list features create learning attributes from the specified feature for each annotation of the specified type in the range specified around the instance. For example, -5 to 5 creates a total of ten attributes (assuming they are available to be had). This feature type can also be used to create positional features; for example on the previous or subsequent annotation. (This way of specifying positional features differs from that in the Batch Learning PR.) This type of feature essentially allows aspects of sequence learning to be included in a non-sequence learner. You might include the string of the token in position 2, for example, and find that the learner detects people more easily because verbs often appear in that position that are unique to people, such as "said". Such features would not be terribly useful if you were using a sequence learner such as Mallet's CRF, but for other learners, this information might be useful. Nominal and numeric types are both supported.


Learners and their Parameters
-----------------------------

LibSVM:

LibSVM can be used as a standard classifier in NER or classification mode. Probabilities are included by default, and are "real" probabilities. However, the resulting model is a little slower than if probabilities aren't calculated. If you are happy to use classifications without information about the probability of that classification being correct, and need your learner to be faster, you can turn off probability generation using "-b" as described below.

The full range of parameters are supported, which can be specified in the "learnerParams" field as a space-separated flagged sequence, as described in the LibSVM documentation:

-s svm_type : set type of SVM (default 0)
	0 -- C-SVC
	1 -- nu-SVC
	2 -- one-class SVM
	3 -- epsilon-SVR
	4 -- nu-SVR
-t kernel_type : set type of kernel function (default 2)
	0 -- linear: u'*v
	1 -- polynomial: (gamma*u'*v + coef0)^degree
	2 -- radial basis function: exp(-gamma*|u-v|^2)
	3 -- sigmoid: tanh(gamma*u'*v + coef0)
-d degree : set degree in kernel function (default 3)
-g gamma : set gamma in kernel function (default 1/num_features)
-r coef0 : set coef0 in kernel function (default 0)
-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)
-n nu : set the parameter nu of nu-SVC, one-class SVM, and nu-SVR (default 0.5)
-p epsilon : set the epsilon in loss function of epsilon-SVR (default 0.1)
-m cachesize : set cache memory size in MB (default 100)
-e epsilon : set tolerance of termination criterion (default 0.001)
-h shrinking: whether to use the shrinking heuristics, 0 or 1 (default 1)
-b probability_estimates: whether to train a SVC or SVR model for probability estimates, 0 or 1 (default 0)
-wi weight: set the parameter C of class i to weight*C, for C-SVC (default 1)


Mallet CRF:

Mallet CRF is the only sequence learner currently integrated. No parameters are used with Mallet CRF. You will need to set the sequenceSpan to something sensible; one learning instance will be generated for each sequence anotation. For example, if you are doing NER and are using "token" as instance, then you might give a sequence span of "sentence", because tokens fall into meaningful patterns within sentences, but not so much across sentence boundaries. Mallet CRF will then learn the material in sentence-long chunks. You need to have sentence annotations prepared on your document, as well as the tokens.

A sequence classifier is an appropriate choice in any context where your instances tend to fall into predictable sequences. It's good for named entity detection, for example, because named entities are often predictable sequences, such as descriptions of symptoms in clinical applications, and the contexts in which they appear are also often meaningful sequences. However, it wouldn't be so good for document classification because documents don't tend to form a meaningful sequence.


Mallet classification:

A number of Mallet classification algorithms are integrated, and the following parameters are available. In each case, parameters are given space separated and unflagged in the order specified:

Balanced Winnow:
 epsilon (double, default 0.5)
 delta (double, default 0.1)
 max iterations (int, default 30)
 cooling rate (double, default 0.5)

C45:
 max depth (int)

Decision Tree:
 max depth (int)
 
Max Ent GE Range, Max Ent GE, Max Ent PR: These all take an array of constraints. This isn't currently supported.

Max Ent:
 gaussian prior (double, a parameter to avoid overtraining. 1.0 is the default value.)
 max iterations (int. I have coded this in but it is possible that Mallet still doesn't use it.)

MC Max Ent:
 The following configurations only are supported:
  gaussianPriorVariance (double, a parameter to avoid overtraining)
 OR
  gaussianPriorVariance (double)
  useMultiConditionalTraining (boolean)
 OR
  hyperbolicPriorSlope (double)
  hyperbolicPriorSharpness (double)
 OR no arguments.

Naive Bayes, Naive Bayes EM: These don't take any parameters.

Winnow:
 a (double)
 b (double)
 nfact (double, optional)


Weka classification:

A number of Weka classification algorithms are included, and parameters supported as specified in the Weka documentation.

Weka's additive regression: this algorithm differs from all other algorithms included in that it takes a numeric class. This algorithm will not work in NER mode and it won't work if your class feature is nominal. At apply time, a numeric class is added to output instances and the confidence field is left blank.


Evaluation
----------

Evaluation is an essential part of machine learning work. It is vital to know how well your system performs, and this cannot be discerned from simply looking at a few documents. Performance statistics should be generated over a corpus large enough to give an indicator of the likely success of the approach in an application context.

Evaluation modes integrated in the PR include both hold-out and cross-validation, and runtime parameters allow you to specify the number of folds for cross-validation, or the percentage to hold out for hold out evaluation. To perform evaluation, select the relevant mode.

Evaluation modes simply return basic classification accuracy. Note that classification accuracy alone may give a misleading impression of task success for highly imbalanced datasets, such as are common in NLP. For example, if you have two named entities in 500 "token" training instances, then the learner could easily set all instances to "false" and appear to have a success in excess of 99%. A correct task evaluation would involve calculating precision and recall in finding these named entities. The integrated evaluation modes wouldn't therefore be suitable for publishing results about the success of the approach in finding named entities, or gaining a real feel for how successful your system is. Suggested use of evaluation modes is to facilitate parameter tuning in making your learner the best it can be. You would then need to perform a separate task evaluation.

GATE offers a range of more advanced evaluation functionality for determining actual task success. For example, you could separate your corpus into training and test sets, and having applied your learner to the test set, use Corpus QA to investigate its performance. Another option would be to save separate applications for training and applying your learner, then use the Cross Validation PR to determine performance.

In the case that you want to try several combinations of parameters, the following approach would be sensible: create several XGAPP files with different learner parameters, then use a script to run them each in turn in cross-validation or hold-out mode (depending on the time available--cross-validation takes longer to run) in order to obtain the best parameter combination. Then evaluate performance of the best learner on your task using Corpus QA. 


Example: Classification
-----------------------

For a sentence classification task, your corpus is prepared using ANNIE, so tokens and sentences are available in the default annotation set. Each sentence (in the default annotation set) is amended to include a "class" feature of "true", "false" or "unknown". The following feature specification is provided, indicating that bigrams of strings of tokens are to be used, along with unigrams of categories of token.

<ML-CONFIG>

<NGRAM>
<NUMBER>2</NUMBER>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>
</NGRAM>

<NGRAM>
<NUMBER>1</NUMBER>
<TYPE>Token</TYPE>
<FEATURE>category</FEATURE>
</NGRAM>

</ML-CONFIG>

Runtime parameters are set as follows (ones not listed aren't relevant):

classFeature: class
classType: Sentence
featureSpecURL: (give the location to which you saved the feature spec file)
inputASName: blank (default)
instanceName: Sentence
mode: CLASSIFICATION
outputASName: (leave it as default)
saveDirectory: (your choice, but you have to say something)
trainingAlgo: your choice! Anything other than additive regression will work. CRF isn't appropriate to the task.

You proceed to train and apply your model by selecting the appropriate operations and running the application. For further performance improvements, consider using a gazetteer to annotate key words of particular relevance, then include those as features. Try some different parameters with your learner, if different parameters are available.


Example: Named Entity Recognition
---------------------------------

For a named entity recognition task, your corpus is prepared using ANNIE, so tokens are available. You are trying to find locations in the text, so in the default annotation set you have human-annotated "Location" annotations. Your feature specification file is as follows, indicating that you want to use the string of token and a range of categories of token spanning from minus one to two tokens before and after the start of the current instance token (the current instance counts as position one, so this is a total of three).

<ML-CONFIG>

<ATTRIBUTE>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>
<DATATYPE>nominal</DATATYPE>
</ATTRIBUTE>

<ATTRIBUTELIST>
<TYPE>Token</TYPE>
<FEATURE>category</FEATURE>
<DATATYPE>nominal</DATATYPE>
<FROM>-1</FROM>
<TO>2</TO>
</ATTRIBUTELIST>

</ML-CONFIG>

Runtime parameters are set as follows (ones not listed aren't relevant):

classType: Location
featureSpecURL: (give the location to which you saved the feature spec file)
inputASName: blank (default)
instanceName: Token
mode: NAMED_ENTITY_RECOGNITION
outputASName: (leave it as default)
saveDirectory: (your choice, but you have to say something)
sequenceSpan: Sentence (if using CRF, ignored otherwise)
trainingAlgo: your choice! CRF is a nice one for this kind of task LibSVM is a good all-rounder.

You proceed to train and apply your model by selecting the appropriate operations and running the application. For further performance improvements, consider different learner parameters and more features.

Hints, Tips and FAQs
--------------------

Can I export data for use in another application?
  Yes. ARFF is available as an export format, and you can select this as an operation.

I want the PR to write the output classes onto existing annotations at apply time, but instead it just makes duplicate tokens ..?
  Forthcoming.

I generated some ARFF from a dataset earlier and now I want to generate some more ARFF from different data but with a compatible feature space ..?
  Forthcoming.

I want to set weights for my classes in LibSVM but it wants class numbers, and I don't know what they are ..?
  Forthcoming.

I ran the PR in NER mode to apply my model, but it didn't generate any NEs. I don't understand what happened ..?
  Try setting it to classification mode and re-applying. It will complain if you try to apply an NER model in classification mode, but it will do it anyway. You will then see "behind the scenes". In NER mode, the learner actually learns three classes; beginning, inside and outside (of the NE type you are trying to learn). If you apply an NER model in classification mode, it will make annotations for each instance, classified into one of those three with a confidence attached, but it won't take the second step of turning them into NE annotations and removing the instance annotations. So now you can have a look and see what kind of classifications you are getting, and what their confidences are. It may be, for example, that you need to set your confidence threshold differently, or it may reveal that your learner isn't managing to find beginnings, insides and outsides very well at all, which might indicate inadequate features.

My learner is slow/isn't working very well/etc. ..?
  Export to ARFF and have a look at the training set it generates. Looking at the ARFF can often reveal if the feature set you are generating isn't what you intended, as well as deepening your understanding of what you are doing.

You mention a Cross Validation PR above but I can't find it ..?
  Forthcoming!



